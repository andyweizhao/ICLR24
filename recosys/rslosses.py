"""Loss functions for CF with support for optional negative sampling."""
import abc
import torch


class LossFunction(abc.ABC):
    """Abstract loss function for CF embeddings."""

    @abc.abstractmethod
    def calculate_loss(self, model, input_batch):
        """
        :param model: CF embedding model.
        :param input_batch: Tensor of size batch_size x 3 containing input pairs: (head, relation, tail)
        :return: Average loss within the input_batch.
        """
        pass


class NegativeSampleLoss(LossFunction, abc.ABC):
    """Abstract loss with negative sampling.
    Input batch is always of the form (head, tail).
    It will run the model with positive samples of the form: (head, relation, tail) and
    with negative samples of the form: (head, corrupted_tail).
    Corrupted tails are generated by taking a uniform random sample over all the entities
    """
    def __init__(self, ini_neg_index, end_neg_index, args):
        """
        :param ini_neg_index: lower index to generate negative samples
        :param end_neg_index: higher index to generate negative samples. Pre: end_neg_index > ini_neg_index
        :param args: flags
        """
        super().__init__()
        self.ini_neg_index = ini_neg_index
        self.end_neg_index = end_neg_index
        self.neg_sample_size = args.neg_sample_size

    def build_negative_input_batch(self, input_batch):
        """From a batch x 2 input_batch tensor with (head, tail) builds a
        batch x 2 input batch of the form (head, corrupted_tail)"""
        head = torch.unsqueeze(input_batch[:, 0], 1)            # b x 1
        corrupted_tail = torch.randint(low=self.ini_neg_index, high=self.end_neg_index,
                                       size=(len(input_batch) * self.neg_sample_size, 1),
                                       device=input_batch.device)
        noisy_tail = torch.remainder(corrupted_tail + 1, self.end_neg_index).long()
        corrupted_tail = torch.where(head == corrupted_tail, noisy_tail, corrupted_tail)
        return torch.cat((head, corrupted_tail), dim=-1)


class BCELoss(NegativeSampleLoss):
    """Binary Cross Entropy loss.
    The probability of the triplet being true is given by sigma(score(head, relation, tail))
    This loss aims to maximize the probability of positive samples and minimize the one of
    negative samples.
    """
    def __init__(self, ini_neg_index, end_neg_index, args):
        super().__init__(ini_neg_index, end_neg_index, args)
        self.bce_from_logits = torch.nn.BCEWithLogitsLoss()

    def calculate_loss(self, model, input_batch):
        neg_input_batch = self.build_negative_input_batch(input_batch)
        total_input_batch = torch.cat((input_batch, neg_input_batch), dim=0)

        true_labels = torch.ones((len(input_batch), 1))
        neg_labels = torch.zeros((len(neg_input_batch), 1))
        total_labels = torch.cat((true_labels, neg_labels), dim=0).to(input_batch.device)

        scores = model(total_input_batch)
        
        loss = self.bce_from_logits(torch.unsqueeze(scores, 1), total_labels)
        return loss


class HingeLoss(NegativeSampleLoss):
    """Hinge triple loss based on scoring positive samples higher than negative samples."""
    def __init__(self, ini_neg_index, end_neg_index, args):
        super().__init__(ini_neg_index, end_neg_index, args)
        self.margin = args.hinge_margin

    def calculate_loss(self, model, input_batch):
        pos_score = model(input_batch)
        loss = 0
        for _ in range(self.neg_sample_size):
            neg_input_batch = self.build_negative_input_batch(input_batch)
            neg_score = model(neg_input_batch)
            loss = loss + torch.mean(torch.nn.functional.relu(self.margin - pos_score + neg_score))
        return loss
